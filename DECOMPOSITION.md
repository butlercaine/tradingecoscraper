# DECOMPOSITION — PROJ-2026-0207-tradingecoscraper

**Generated by:** project-lead
**Date:** 2026-02-07
**Project:** Trading Economics Homepage Scraper

---

## Milestone 1: Core Scraper + Parsers + Models

### TASK_001_python-dev — Project Setup + Configuration
Create Python project structure with `config.py` containing HTTP settings, selectors, User-Agent rotation, rate limiting (5s delay), and retry configuration. Set up `requirements.txt` with all dependencies.

### TASK_002_python-dev — HTTP Scraper Core + Robots.txt Compliance
Build `scraper.py` with httpx client, async support, retry logic (3 attempts), rate limiting, robots.txt parsing/checking, and error handling. Exports `scrape_page(url: str) -> str`.

### TASK_003_python-dev — Pydantic Data Models
Create `models.py` with Pydantic models for all scraped data: MarketInstrument, MacroIndicator, NewsArticle, and TradingEconomicsOutput. Include validation for all fields.

### TASK_004_python-dev — Market Panels Parser
Build `parsers/markets.py` to parse 6 market panels (Commodities, Stock Indexes, Major Stocks, Forex, Government Bonds, Crypto) from HTML. Each panel returns list of `MarketInstrument` objects.

### TASK_005_python-dev — Country Macro Indicators Parser
Build `parsers/macro.py` to parse the 13-country macro indicators matrix from HTML. Extract 9 fields per country (GDP, Inflation, Unemployment, etc.) into `MacroIndicator` objects.

### TASK_006_python-dev — Headlines Parser
Build `parsers/headlines.py` to parse 3 headline news articles from HTML. Extract title, summary, timestamp, and URL into `NewsArticle` objects.

### TASK_007_python-dev — Main Scraper Orchestrator
Create `scraper.py` main function or `__main__.py` that orchestrates: fetch HTML → parse markets → parse macro → parse headlines → validate with Pydantic → output structured dict.

### TASK_008_python-dev — Playwright Fallback for JS-Rendered Content
Implement Playwright fallback in `scraper.py` for panels that require JS rendering. Detect JS-rendered panels via selector check; if static HTML fails, use Playwright to get rendered page.

### TASK_009_qa-engineer — M1 QA Validation
Validate M1 deliverables: 6 market panels parse correctly, 13-country macro matrix parses, 3 headlines extracted, Pydantic validation passes, retry logic works, Playwright fallback triggers.

### TASK_010_scribe — M1 Decision Log
Document key decisions: HTTP client choice, HTML parsing strategy, Playwright fallback trigger condition, Pydantic model design decisions, selector management approach.

### TASK_011_git-commit-agent — M1 Commit + Push
Initialize git repo, commit all M1 files, push to `butlercaine/tradingecoscraper`. Tag `v1.0.0-m1`.

---

## Milestone 2: Testing + JSON Output + Release

### TASK_012_python-dev — HTML Fixtures + Unit Tests
Create `tests/fixtures/homepage.html` with sample HTML for testing. Write `tests/test_parsers.py` with ≥80% coverage testing all parsers with fixtures.

### TASK_013_python-dev — JSON Output Sample
Run the scraper to generate `tradingeconomics.json` sample output. Ensure output matches Pydantic model schema and is valid JSON.

### TASK_014_python-dev — README + Documentation
Create `README.md` with: installation instructions, usage examples, configuration options, API reference, and architecture overview.

### TASK_015_qa-engineer — M2 QA Validation
Validate M2 deliverables: ≥80% test coverage, all parser tests pass, JSON valid, README complete, Pydantic validation passes.

### TASK_016_scribe — M2 Decision Log + Project Summary
Document test fixture strategy, JSON schema design, selector changes from real scrape. Add project summary.

### TASK_017_git-commit-agent — M2 Commit + Push + Release
Commit M2 files, push to `butlercaine/tradingecoscraper`, create release tag `v1.0.0` with changelog.

---

## Dependency Graph

```
TASK_001 → TASK_002 → TASK_004 → TASK_007 → TASK_008
   ↓           ↓           ↓
TASK_003     TASK_005     TASK_006
              ↓            ↓
             TASK_009 (M1 QA)
                ↓
            TASK_010 (M1 Scribe)
                ↓
            TASK_011 (M1 Commit)
                ↓
         TASK_012 → TASK_013 → TASK_014
                ↓            ↓
             TASK_015 (M2 QA)
                ↓
            TASK_016 (M2 Scribe)
                ↓
            TASK_017 (M2 Commit + Release)
```

---

## Notes

- **Data Point Clarification:** Spec says "100 data points" but math shows ~200 values. Treat 100 as minimum; scrape all available.
- **Playwright:** Requires `playwright install chromium` step (add to README).
- **Rate Limiting:** 5s delay configured to prevent ToS violations.
